"""
This code is generated by Ridvan Salih KUZU
LAST EDITED:  20.06.2022
ABOUT SCRIPT:
It runs few shot learning on given CLIP models and makes hyperparameter fine-tuning
"""

import clip
from clip.clip_data_loader import CLIPDataloader
from clip.losses import SymmetricCrossEntropyLoss
from clip.utils import AvgMeter, get_lr
from clip.downstream_task import TaskType

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, classification_report,confusion_matrix, ConfusionMatrixDisplay
import torch
from tqdm import tqdm
import optuna
from optuna.samplers import TPESampler
import argparse
import warnings
import pandas as pd
import matplotlib.pyplot as plt
import joblib
import gc
import os

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def main(args):

    if not os.path.exists(args.log_dir):
        os.makedirs(args.log_dir)


    available_models = clip.available_models()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    selected_model_name = available_models[-2] #ViT-L/14


    def objective(trial):
        '''
            THIS FUNCTION DEFINES THE OBJECTIVE FUNCTION FOR BAYESIAN HYPERPARAMETER TUNING
            :param trial: trial object of bayesian optimization
            :return: returns weighted F1 score to be maximized
        '''

        gc.collect()

        print(f"INFO: Trial number: {trial.number}\n")

        learning_rate = trial.suggest_categorical('learning_rate', args.learning_rate)
        trainable_layers = trial.suggest_categorical('trainable_layers', args.trainable_layers)
        downstream_task = trial.suggest_categorical("downstream_task", args.downstream_task)

        output_file_template = '{}_{}_{}_{}_{}'.format(args.log_dir,learning_rate,trainable_layers,downstream_task,trial.number)

        # TODO: loss_temperature = trial.suggest_float("temperature", args.loss_temperature[0], args.loss_temperature[1], args.loss_temperature[0])

        downstream_task = TaskType(args.downstream_task.index(downstream_task))
        model, default_transform = clip.load(selected_model_name, device, downstream_task=downstream_task, class_num=args.num_classes)


        if 'ViT' in selected_model_name:
            print('INFO: training model name: {}'.format(selected_model_name))
            set_vit_trainable_layers(model, trainable_layers, downstream_task)
        else:
            raise NotImplementedError
            # since ViT models perform better in given linear probe evaluation in '01_b_clip_linear_probe_tuning.py'
            # we skip the implementation for Resnet based models

        data_loader = CLIPDataloader(args.train_data, args.test_data, model.visual.input_resolution)

        train_validate(model, data_loader.get_data_loader('train'), data_loader.get_data_loader('valid'), learning_rate, downstream_task, output_file_template, device)

        model, transform = clip.load('{}_model_best.pth.tar'.format(output_file_template),device,downstream_task=downstream_task,class_num=args.num_classes)

        if downstream_task == TaskType.ARC_HEAD or downstream_task == TaskType.DEFAULT:
            y_true, y_pred = inference_linear_probe(model, data_loader.get_data_loader('train'), data_loader.get_data_loader('valid'), device)

        elif downstream_task == TaskType.MLP_HEAD or downstream_task == TaskType.ARC_MLP_HEAD:
            y_true, y_pred = inference_mlp_head(model, data_loader.get_data_loader('train'),data_loader.get_data_loader('valid'), device)

        monitored_score = inference_reporting(y_true, y_pred, data_loader.classes, output_file_template)

        return monitored_score

    study = optuna.create_study(sampler=TPESampler(), direction='maximize')

    log_file=args.log_dir+'optimization_logs.pkl'
    if os.path.isfile(log_file):
        study = joblib.load(log_file)

    study.optimize(objective, n_trials=args.n_trials, gc_after_trial=True)
    joblib.dump(study, log_file)


def train_validate(model, train_loader, valid_loader, learning_rate, downstream_task, output_template, device):
    '''
        THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
        :param model: CLIP model to be fine-tuned
        :param train_loader: data loader for training files
        :param valid_loader: data loader for validation files
        :param learning_rate: learning rate for optimization
        :param downstream_task: one of few-shot learning tasks 'DEFAULT', 'ARC_HEAD','MLP_HEAD','ARC_MLP_HEAD'
        :param output_template: file name template to save the fine-tuned models and inference statistics
        :param device: GPU or CPU device
        :return:
    '''

    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), betas=(0.9,0.98),eps=1e-6, lr=learning_rate)  # weight_decay=0.0001
    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

    if downstream_task == TaskType.ARC_HEAD or downstream_task == TaskType.DEFAULT:
        loss_criterion = SymmetricCrossEntropyLoss().to(device)
    else:
        loss_criterion = torch.nn.CrossEntropyLoss(reduction='mean').to(device)

    step = "epoch"
    best_loss = float('inf')
    for epoch in range(args.total_epoch):
        print(f"Epoch: {epoch + 1}")
        model.train()

        train_epoch(model, train_loader, optimizer, lr_scheduler, loss_criterion, step, device)
        model.eval()
        with torch.no_grad():
            valid_loss, image_batch, text_batch = valid_epoch(model, valid_loader, loss_criterion, device)

        if valid_loss.avg < best_loss:
            best_loss = valid_loss.avg
            torch.jit.save(torch.jit.trace(model, (image_batch, text_batch)),'{}_model_best.pth.tar'.format(output_template))
            # torch.save(model.state_dict(), "model_best.pth.tar")
            print("INFO: Best Model saved into " + '{}_model_best.pth.tar'.format(output_template))

        lr_scheduler.step(valid_loss.avg)


def train_epoch(model, train_loader, optimizer, lr_scheduler, loss_criterion, step, device):
    '''
         THIS FUNCTION RUNS THE SINGLE TRAINING EPOCHS
         :param model: CLIP model to be fine-tuned
         :param train_loader: data loader for training files
         :param optimizer: model optimizer object
         :param lr_scheduler: scheduler object to update the learning rate for optimization
         :param loss_criterion: loss objective function to be utilized for backprop
         :param step: epoch step number
         :param device: GPU or CPU device
         :return: returns average training loss
    '''

    loss_meter = AvgMeter()
    tqdm_object = tqdm(train_loader, total=len(train_loader))
    for image_batch, text_batch, label_batch in tqdm_object:
        output = model(image_batch.to(device), text_batch.to(device), label_batch.to(device))

        loss = loss_criterion(output,label_batch.to(device))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if step == "batch":
            lr_scheduler.step()

        count = image_batch.size(0)
        loss_meter.update(loss.item(), count)

        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))
    return loss_meter


def valid_epoch(model, valid_loader, loss_criterion, device):
    '''
         THIS FUNCTION RUNS THE SINGLE TRAINING EPOCHS
         :param model: CLIP model to be fine-tuned
         :param valid_loader: data loader for validation files
         :param loss_criterion: loss objective function to be utilized for backprop
         :param step: epoch step number
         :param device: GPU or CPU device
         :return: returns the tuple of ( average validation loss, an image batch example, a text batch example).
                  image and text batches are used as template to save the model if it reaches the optimum
    '''

    loss_meter = AvgMeter()

    tqdm_object = tqdm(valid_loader, total=len(valid_loader))
    for image_batch, text_batch, label_batch in tqdm_object:
        output = model(image_batch.to(device), text_batch.to(device), label_batch.to(device))
        loss = loss_criterion(output,label_batch.to(device))
        count = image_batch.size(0)
        loss_meter.update(loss.item(), count)

        tqdm_object.set_postfix(valid_loss=loss_meter.avg)
    return loss_meter, image_batch.to(device), text_batch.to(device)


def inference_linear_probe(model, train_loader, eval_loader, device):
    '''
         THIS FUNCTION RUNS THE LINEAR PROBE BASED INFERENCE
         :param model: CLIP model to be fine-tuned
         :param train_loader: data loader for training files
         :param eval_loader: data loader for evaluation files
         :param device: GPU or CPU device
         :return: returns the tuple of ( ground truth labels, predicted labels) for the evaluation samples
    '''

    train_features,_, y_train = get_features_projections(train_loader, model, device)
    test_features,_, y_test = get_features_projections(eval_loader, model, device)

    classifier = LogisticRegression(random_state=0, C=0.8, max_iter=1000, verbose=0,
                                    class_weight='balanced', n_jobs=-1)
    classifier.fit(train_features, y_train)
    y_pred = classifier.predict(test_features)

    return y_test,y_pred


def inference_mlp_head(model, train_loader, eval_loader, device):
    '''
         THIS FUNCTION RUNS THE MLP BASED INFERENCE
         :param model: CLIP model to be fine-tuned
         :param train_loader: data loader for training files (not required here, but for compatibility purposes with other inference functions in this project)
         :param eval_loader: data loader for evaluation files
         :param device: GPU or CPU device
         :return: returns the tuple of ( ground truth labels, predicted labels) for the evaluation samples
    '''
    #_,train_proj, y_train = get_features_projections(train_loader, model, device)
    _,test_proj, y_test = get_features_projections(eval_loader, model, device)

    return y_test, test_proj


def inference_reporting(y_true, y_pred, class_names, output_template):
    '''
         THIS FUNCTION GENERATES THE REPORTING FILES FOR PERFORMANCE COMPARISON
         :param y_true: ground truth labels
         :param y_pred: predicted labels
         :param class_names: name of the classes in training and evaluation folders
         :param output_template: file name template to save the fine-tuned models and inference statistics
         :return: returns weighted F1 score
    '''

    report = classification_report(y_true, y_pred, target_names=class_names)
    print(report)

    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
    report_df = pd.DataFrame(report).transpose()
    report_df.to_csv('{}_classification_report.csv'.format(output_template), index=True)

    fig, ax = plt.subplots(figsize=(9, 8))
    cm = confusion_matrix(y_true, y_pred, normalize='true')
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(xticks_rotation='vertical', ax=ax)
    fig.savefig('{}_confusion_matrix.jpg'.format(output_template))


    return f1_score(y_true, y_pred, average='weighted')


def get_features_projections(data_loader, model, device):
    '''
        THIS FUNCTION EXTRACTS THE FEATURES OF GIVEN IMAGES BY USING IMAGE_ENCODER METHOD OF THE GIVEN MODEL
        :param data_loader: data loader in CLIPDataloader object type
        :param model: CLIP model to be exploited for feature extraction
        :param device: GPU or CPU device for placing the model
        :return: returns the tuple of (extracted features, projected features to class IDs, labels)
    '''

    all_features = []
    all_projections = []
    all_labels = []

    with torch.no_grad():
        for image_batch, text_batch, label_batch in tqdm(data_loader):
            features = model.encode_image(image_batch.to(device))
            projections = model.project_image(features)
            projections = torch.argmax(projections, dim=-1)

            all_features.append(features)
            all_projections.append(projections)
            all_labels.append(label_batch)

    return torch.cat(all_features).cpu().numpy(), torch.cat(all_projections).cpu().numpy(), torch.cat(all_labels).cpu().numpy()


def set_vit_trainable_layers(model, trainable_layers, downstream_task):
    '''
        THIS FUNCTION SET ONLY GIVEN LAYERS NAMES AND INDICES AS TRAINABLE AND FREEZE THE REST OF THE MODEL
        :param model: CLIP model to be exploited for feature extraction
        :param trainable_layers: total number of trainable layers to be activated in the final part of the architecture
        :param downstream_task: one of few-shot learning tasks 'DEFAULT', 'ARC_HEAD','MLP_HEAD','ARC_MLP_HEAD'
        :return:
    '''

    for idx, param in enumerate(model.parameters()):
        param.requires_grad = False

    for idx, param in enumerate(model.visual.transformer.resblocks[trainable_layers:].parameters()):
        param.requires_grad = True
    for idx, param in enumerate(model.visual.ln_post.parameters()):
        param.requires_grad = True
    for idx, param in enumerate(model.transformer.resblocks[trainable_layers:].parameters()):
        param.requires_grad = True
    for idx, param in enumerate(model.ln_final.parameters()):
        param.requires_grad = True

    if downstream_task == TaskType.ARC_MLP_HEAD:
        for idx, param in enumerate(model.external_image_arch_header.parameters()):
            param.requires_grad = True
        for idx, param in enumerate(model.external_image_mlp_header.parameters()):
            param.requires_grad = True

    elif downstream_task == TaskType.ARC_HEAD:
        for idx, param in enumerate(model.external_image_arch_header.parameters()):
            param.requires_grad = True
        for idx, param in enumerate(model.external_text_arch_header.parameters()):
            param.requires_grad = True

    elif downstream_task == TaskType.MLP_HEAD:
        for idx, param in enumerate(model.external_image_mlp_header.parameters()):
            param.requires_grad = True



if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--train-data', type=str, default='data/coco_crops_few_shot/train/')
    parser.add_argument('--test-data', type=str, default='data/coco_crops_few_shot/test/')
    parser.add_argument('--log-dir', type=str, default='logs/')
    parser.add_argument('--total-epoch', type=int, default=30)
    parser.add_argument('--num-classes', type=int, default=8)
    parser.add_argument('--trainable-layers', type=int, nargs='+', default=[-1, -2, -3, -4, -5])
    parser.add_argument('--n-trials', type=int, default=100)
    parser.add_argument('--learning-rate', type=float, nargs='+', default=[1e-3, 1e-4, 2.5e-4, 5e-4, 1e-5, 2.5e-5, 5e-5, 1e-6, 2.5e-6, 5e-6])
    parser.add_argument('--loss-temperature', type=float, nargs='+', default=[1e-3, 1e-1])
    parser.add_argument('--downstream-task', type=str, nargs='+', default=['DEFAULT', 'ARC_HEAD','MLP_HEAD','ARC_MLP_HEAD']) #'MLP_HEAD','ARC_MLP_HEAD'



    args = parser.parse_args()

    main(args)
